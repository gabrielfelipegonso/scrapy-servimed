# üì¶ Scrapy Servimed

Crawler desenvolvido com [Scrapy](https://scrapy.org/) para fazer login no portal **Servimed**, buscar um id de cliente ativo e coletar informa√ß√µes de produtos gerando dinamicamente um .

---

## ‚ú® Funcionalidades
- Login com usu√°rio e senha.  
- Descoberta autom√°tica do **Client ID** v√°lido.  
- Pagina√ß√£o autom√°tica na API de produtos.  
- Extra√ß√£o de informa√ß√µes de cada produto:
  - **GTIN (EAN)**  
  - **C√≥digo**  
  - **Descri√ß√£o**  
  - **Pre√ßo de f√°brica**  
  - **Estoque**  
- Exporta√ß√£o dos dados em **JSON**, **JSONLines** ou **CSV**.

---

## üóÇ Estrutura do projeto

```
scrapy_servimed/
‚îú‚îÄ‚îÄ scrapy.cfg                  # Configura√ß√£o Scrapy (CLI)
‚îú‚îÄ‚îÄ run_spider.py               # Script de execu√ß√£o standalone
‚îú‚îÄ‚îÄ servimedScraper/
‚îÇ   ‚îú‚îÄ‚îÄ settings.py             # Configura√ß√µes do projeto Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py          # Middleware custom (auth headers/cookies)
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jwt.py              # Fun√ß√£o para decodifica√ß√£o JWT
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requests.py         # Builders de JsonRequest (login, clientes, produtos)
‚îÇ   ‚îî‚îÄ‚îÄ spiders/
‚îÇ       ‚îî‚îÄ‚îÄ products.py         # Spider principal
‚îî‚îÄ‚îÄ tests/                      # Implementar os testes
```

---

## ‚öôÔ∏è Requisitos

- Python 3.12+  
- [Poetry](https://python-poetry.org/) (recomendado) ou `pip`  
- Scrapy >= 2.13  

---

## üöÄ Instala√ß√£o

Clone o reposit√≥rio e instale as depend√™ncias:

```bash
git clone https://github.com/gabrielfelipegonso/scrapy-servimed
cd scrapy-servimed
poetry install
```

Ou, sem Poetry:

```bash
pip install -r requirements.txt
```

---

## üîë Configura√ß√£o de credenciais

As credenciais podem ser fornecidas de duas formas:

### 1. Via linha de comando
```bash
python run_spider.py --usuario "meu@email.com" --senha "minha_senha"
```

### 2. Via vari√°veis de ambiente
```bash
# Linux/macOS
export SERVIMED_USER="meu@email.com"
export SERVIMED_PASS="minha_senha"

# Windows (PowerShell)
$env:SERVIMED_USER="meu@email.com"
$env:SERVIMED_PASS="minha_senha"
$env:SERVIMED_SALE_TYPE= '1' ou "2" 
```

---

## üï∑Ô∏è Executando o Spider

Ap√≥s ativar o ambiente virtual e acessar no seu terminal a pasta que cont√©m o script run_spider.py (servimedScraper\servimedScraper)

### Op√ß√£o A ‚Äì Usando a CLI do Scrapy
```bash
scrapy crawl products -a usuario="meu@email.com" -a senha="minha_senha" -o produtos.jsonl -t jsonlines
```

### Op√ß√£o B ‚Äì Usando o script `run_spider.py`
```bash
python run_spider.py   --usuario "meu@email.com"   --senha "minha_senha"   --output produtos.jsonl   --format jsonlines   --loglevel INFO
```

---

## üìä Sa√≠da dos dadoss

Por padr√£o, os produtos s√£o exportados em **JSONLines** (`.jsonl`), com um objeto por linha:

```json
{"gtin": "7891234567890", "codigo": "12345", "descricao": "Produto X", "preco_fabrica": 10.5, "estoque": 50}
{"gtin": "7899876543210", "codigo": "67890", "descricao": "Produto Y", "preco_fabrica": 8.0, "estoque": 0}
```

Tamb√©m √© poss√≠vel exportar em JSON √∫nico ou CSV:
```bash
python run_spider.py -o produtos.json -f json
python run_spider.py -o produtos.csv -f csv
```

---
## ‚öôÔ∏è Par√¢metros do run_spider.py

O script run_spider.py permite executar o spider diretamente, sem precisar usar o comando scrapy crawl. Ele aceita diversos par√¢metros para configurar a execu√ß√£o:

| Par√¢metro       | Atalho | Default         | Descri√ß√£o                                                                                                                      |
| --------------- | ------ | ---------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| `--usuario`     | `-u`   | `SERVIMED_USER`  | Usu√°rio de login. Pode ser passado na CLI ou via vari√°vel de ambiente.                                                         |
| `--senha`       | `-p`   | `SERVIMED_PASS`  | Senha de login. Pode ser passada na CLI ou via vari√°vel de ambiente.                                                           |
| `--output`      | `-o`   | `produtos.jsonl` | Caminho do arquivo de sa√≠da. Aceita qualquer extens√£o suportada (`.json`, `.jsonl`, `.csv`).                                   |
| `--format`      | `-f`   | `jsonlines`      | Formato do output (`json`, `jsonlines`, `csv`).                                                                                |
| `--loglevel`    |        | `INFO`           | N√≠vel de log do Scrapy (`DEBUG`, `INFO`, `WARNING`, `ERROR`).                                                                  |
| `--concurrency` |        | `8`              | N√∫mero m√°ximo de requisi√ß√µes concorrentes (`CONCURRENT_REQUESTS`).                                                             |
| `--delay`       |        | `0.1`            | Atraso (em segundos) entre requisi√ß√µes (`DOWNLOAD_DELAY`).                                                                     |
| `--saleType`    | `-s`   | `1`              | Tipo de venda para as requisi√ß√µes de produtos: `0` (√† vista) ou `1` (a prazo). Pode ser definido via env `SERVIMED_SALE_TYPE`. |

## üåç Vari√°veis de Ambiente

Al√©m dos par√¢metros na linha de comando, voc√™ pode configurar o spider atrav√©s de vari√°veis de ambiente:

| Vari√°vel             | Equivalente CLI | Descri√ß√£o                                     |
| -------------------- | --------------- | --------------------------------------------- |
| `SERVIMED_USER`      | `--usuario`     | Usu√°rio de login do portal Servimed.         |
| `SERVIMED_PASS`      | `--senha`       | Senha de login do portal Servimed.                               |
| `SERVIMED_SALE_TYPE` | `--saleType`    | Tipo de venda (`0` = √† vista, `1` = a prazo). |

## üìù Exemplos completos de execu√ß√£o
### 1. Executando com credenciais direto na CLI

```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha
```
### 2. Alterando o formato de sa√≠da
#### Salvando em JSON
```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha -o produtos.json -f json
```
#### Salvando em CSV
```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha -o produtos.csv -f csv
```

### 3;Usando vari√°veis de ambiente (sem passar --usuario/--senha)



## üõ† Boas pr√°ticas implementadas

- **Separa√ß√£o de responsabilidades**:
  - Spider cuida apenas do fluxo de scraping.
  - Middleware injeta headers/cookies de autentica√ß√£o.
  - Utils centralizam a cria√ß√£o de requests (login, clientes, produtos).
- **Uso de `JsonRequest`** para evitar erros de serializa√ß√£o.  
- **State centralizado** (`self.state`) para guardar tokens, cookies e IDs.  
- **Script standalone (`run_spider.py`)** para execu√ß√£o automatizada e integra√ß√£o.  
- **Exporta√ß√£o configur√°vel** (formato/arquivo via par√¢metros).  
- **Compat√≠vel com Scrapy 2.13+** (`async def start`).  

---


