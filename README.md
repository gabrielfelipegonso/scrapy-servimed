# üì¶ Scrapy Servimed

Crawler desenvolvido com [Scrapy](https://scrapy.org/) para fazer login no portal **Servimed**, buscar um id de cliente ativo e coletar informa√ß√µes de produtos gerando dinamicamente um .

## üîç Observa√ß√µes acerca do teste

Agrade√ßo a oportunidade de participar do processo seletivo, a prova foi muito bem feita e detalhada e gostei do processo de construir este projeto.

Fiz as requisi√ß√µes 1 a 1 no scraper at√© encontrar uma com lista vazia pois isso tornou mais robusto, no primeiro teste que fiz direto √† API da servimed o c√°lculo deles de quantidade total de produtos n√£o batia com a quantidade de p√°ginas m√°xima.

Sugiro adicionar que o gtin precisa ter pelo menos 8 caracteres e como completar em caso de haver menos. o produto com gtin 450619 tem um c√≥digo de barras menor do que 8 caracteres.Para entregar a prova eu apenas completei com zeros a esquerda na function  parse_products do spider.
---
# üï∑Ô∏è Primeira etapa - Scraper - servimedScraper

## ‚ú® Funcionalidades
- Login com usu√°rio e senha.  
- Descoberta autom√°tica do **Client ID** v√°lido.  
- Pagina√ß√£o autom√°tica na API de produtos.  
- Extra√ß√£o de informa√ß√µes de cada produto:
  - **GTIN (EAN)**  
  - **C√≥digo**  
  - **Descri√ß√£o**  
  - **Pre√ßo de f√°brica**  
  - **Estoque**  
- Exporta√ß√£o dos dados em **JSON**, **JSONLines** ou **CSV**.

---

## üóÇ Estrutura do projeto

```
scrapy_servimed/
‚îú‚îÄ‚îÄ scrapy.cfg                  # Configura√ß√£o Scrapy (CLI)
‚îú‚îÄ‚îÄ run_spider.py               # Script de execu√ß√£o standalone
‚îú‚îÄ‚îÄ servimedScraper/
‚îÇ   ‚îú‚îÄ‚îÄ settings.py             # Configura√ß√µes do projeto Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py          # Middleware custom (auth headers/cookies)
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jwt.py              # Fun√ß√£o para decodifica√ß√£o JWT
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requests.py         # Builders de JsonRequest (login, clientes, produtos)
‚îÇ   ‚îî‚îÄ‚îÄ spiders/
‚îÇ       ‚îî‚îÄ‚îÄ products.py         # Spider principal
‚îî‚îÄ‚îÄ tests/                      # Implementar os testes
```

---

## ‚öôÔ∏è Requisitos

- Python 3.12+  
- [Poetry](https://python-poetry.org/) (recomendado) ou `pip`  
- Scrapy >= 2.13  

---

## üöÄ Instala√ß√£o

Clone o reposit√≥rio e instale as depend√™ncias:

```bash
git clone https://github.com/gabrielfelipegonso/scrapy-servimed
cd scrapy-servimed
poetry install
```

Ou, sem Poetry:

```bash
pip install -r requirements.txt
```

---

## üîë Configura√ß√£o de credenciais

As credenciais podem ser fornecidas de duas formas:

### 1. Via linha de comando
```bash
python run_spider.py --usuario "meu@email.com" --senha "minha_senha"
```

### 2. Via vari√°veis de ambiente
```bash
# Linux/macOS
export SERVIMED_USER="meu@email.com"
export SERVIMED_PASS="minha_senha"

# Windows (PowerShell)
$env:SERVIMED_USER="meu@email.com"
$env:SERVIMED_PASS="minha_senha"
$env:SERVIMED_SALE_TYPE= '1' ou "2" 
```

---

## üï∑Ô∏è Executando o Spider

Ap√≥s ativar o ambiente virtual e acessar no seu terminal a pasta que cont√©m o script run_spider.py (servimedScraper\servimedScraper)

### Op√ß√£o A ‚Äì Usando a CLI do Scrapy
```bash
scrapy crawl products -a usuario="meu@email.com" -a senha="minha_senha" -o produtos.jsonl -t jsonlines
```

### Op√ß√£o B ‚Äì Usando o script `run_spider.py`
```bash
python run_spider.py   --usuario "meu@email.com"   --senha "minha_senha"   --output produtos.jsonl   --format jsonlines   --loglevel INFO
```

---

## üìä Sa√≠da dos dadoss

Por padr√£o, os produtos s√£o exportados em **JSONLines** (`.jsonl`), com um objeto por linha:

```json
{"gtin": "7891234567890", "codigo": "12345", "descricao": "Produto X", "preco_fabrica": 10.5, "estoque": 50}
{"gtin": "7899876543210", "codigo": "67890", "descricao": "Produto Y", "preco_fabrica": 8.0, "estoque": 0}
```

Tamb√©m √© poss√≠vel exportar em JSON √∫nico ou CSV:
```bash
python run_spider.py -o produtos.json -f json
python run_spider.py -o produtos.csv -f csv
```

---
## ‚öôÔ∏è Par√¢metros do run_spider.py

O script run_spider.py permite executar o spider diretamente, sem precisar usar o comando scrapy crawl. Ele aceita diversos par√¢metros para configurar a execu√ß√£o:

| Par√¢metro       | Atalho | Default         | Descri√ß√£o                                                                                                                      |
| --------------- | ------ | ---------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| `--usuario`     | `-u`   | `SERVIMED_USER`  | Usu√°rio de login. Pode ser passado na CLI ou via vari√°vel de ambiente.                                                         |
| `--senha`       | `-p`   | `SERVIMED_PASS`  | Senha de login. Pode ser passada na CLI ou via vari√°vel de ambiente.                                                           |
| `--output`      | `-o`   | `produtos.jsonl` | Caminho do arquivo de sa√≠da. Aceita qualquer extens√£o suportada (`.json`, `.jsonl`, `.csv`).                                   |
| `--format`      | `-f`   | `jsonlines`      | Formato do output (`json`, `jsonlines`, `csv`).                                                                                |
| `--loglevel`    |        | `INFO`           | N√≠vel de log do Scrapy (`DEBUG`, `INFO`, `WARNING`, `ERROR`).                                                                  |
| `--concurrency` |        | `8`              | N√∫mero m√°ximo de requisi√ß√µes concorrentes (`CONCURRENT_REQUESTS`).                                                             |
| `--delay`       |        | `0.1`            | Atraso (em segundos) entre requisi√ß√µes (`DOWNLOAD_DELAY`).                                                                     |
| `--saleType`    | `-s`   | `1`              | Tipo de venda para as requisi√ß√µes de produtos: `0` (√† vista) ou `1` (a prazo). Pode ser definido via env `SERVIMED_SALE_TYPE`. |

## üåç Vari√°veis de Ambiente

Al√©m dos par√¢metros na linha de comando, voc√™ pode configurar o spider atrav√©s de vari√°veis de ambiente:

| Vari√°vel             | Equivalente CLI | Descri√ß√£o                                     |
| -------------------- | --------------- | --------------------------------------------- |
| `SERVIMED_USER`      | `--usuario`     | Usu√°rio de login do portal Servimed.         |
| `SERVIMED_PASS`      | `--senha`       | Senha de login do portal Servimed.                               |
| `SERVIMED_SALE_TYPE` | `--saleType`    | Tipo de venda (`0` = √† vista, `1` = a prazo). |

## üìù Exemplos completos de execu√ß√£o
### 1. Executando com credenciais direto na CLI

```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha
```
### 2. Alterando o formato de sa√≠da
#### Salvando em JSON
```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha -o produtos.json -f json
```
#### Salvando em CSV
```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha -o produtos.csv -f csv
```

### 3;Usando vari√°veis de ambiente (sem passar --usuario/--senha)



## üõ† Boas pr√°ticas implementadas

- **Separa√ß√£o de responsabilidades**:
  - Spider cuida apenas do fluxo de scraping.
  - Middleware injeta headers/cookies de autentica√ß√£o.
  - Utils centralizam a cria√ß√£o de requests (login, clientes, produtos).
- **Uso de `JsonRequest`** para evitar erros de serializa√ß√£o.  
- **State centralizado** (`self.state`) para guardar tokens, cookies e IDs.  
- **Script standalone (`run_spider.py`)** para execu√ß√£o automatizada e integra√ß√£o.  
- **Exporta√ß√£o configur√°vel** (formato/arquivo via par√¢metros).  
- **Compat√≠vel com Scrapy 2.13+** (`async def start`).  

---


# üì® Segunda etapa: Mensageria (RabbitMQ) - servimedQueue

Integra o scraper a um fluxo de mensageria para execu√ß√£o sob demanda e envio do resultado em um √∫nico POST (array JSON).

## üéØ Objetivo

Disparar o scraping por mensagem em fila.

Aguardar o spider concluir.

Enviar todos os itens de uma vez para o endpoint (API_PRODUCTS_URL).

Tratar retries/NACK quando a API estiver indispon√≠vel.

## üß© Componentes
servimedQueue/
‚îú‚îÄ‚îÄ consumers/
‚îÇ   ‚îî‚îÄ‚îÄ consumer_start_scrapy.py   # Conecta no RabbitMQ e escuta a fila de start
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ worker_stream.py           # Callback: roda o spider e faz um POST √∫nico
    ‚îî‚îÄ‚îÄ auth.py                    # AuthClient (password grant) para obter o token


O consumer_start_scrapy usa worker_stream.start_scrap, que:

1) executa run_spider.py,
2) acumula itens emitidos via stdout (JSONL), 
3) realiza um POST com o array completo.

üîÑ Fluxo

Publica-se uma mensagem JSON na fila RABBIT_QUEUE_SCRAPER:

```json
{ "usuario": "email@dominio.com", "senha": "secret", "tipo de venda": 1 }
```

O consumer se inicia e chama o worker

O worker executa o spider e coleta cada linha JSON.

Ao finalizar, o worker POSTA um array para API_PRODUCTS_URL usando AuthClient (Bearer token).

ACK s√≥ ap√≥s POST bem-sucedido.

HTTP 429/5xx ‚Üí NACK requeue

HTTP 4xx ‚Üí NACK sem requeue

‚öôÔ∏è Vari√°veis de ambiente (mensageria)
# RabbitMQ
RABBIT_HOST=localhost
RABBIT_PORT=5672
RABBIT_USER=guest
RABBIT_PASS=guest
RABBIT_QUEUE_SCRAPER=queue.start_scrapy
RABBIT_PREFETCH=1

# Timeouts/heartbeats (√∫til para scrapes longos)
RABBIT_HEARTBEAT=300
RABBIT_BLOCKED_TIMEOUT=600
RABBIT_CONN_ATTEMPTS=5
RABBIT_RETRY_DELAY=5
RABBIT_HEARTBEAT_TICK=1.0  # frequ√™ncia (s) do "tick" de heartbeat no worker

# API destino (POST √∫nico com array)
API_PRODUCTS_URL=https://sua.api.exemplo/produtos

# Auth (password grant) usados por utils/auth.py
API_TOKEN_URL=https://sso.exemplo/oauth/token
API_USERNAME_COTE=usuario
API_PASSWORD_COTE=senha
API_CLIENT_ID_COTE=
API_CLIENT_SECRET_COTE=
API_SCOPE_COTE=

# Scraper
SERVIMED_SALE_TYPE=1
LOG_LEVEL=INFO

# Logs do worker (opcionais)
LOG_EACH_ITEM=false     # true = loga cada item (verboso)
LOG_EVERY_N=0           # >0 = loga a cada N itens

## ‚ñ∂Ô∏è Como rodar o consumer
### op√ß√£o A
python servimedQueue/consumers/consumer_start_scrapy.py

### op√ß√£o B (m√≥dulo)
python -m servimedQueue.consumers.consumer_start_scrapy

## üì® Publicando uma mensagem de teste

```python
import json, os, pika

conn = pika.BlockingConnection(pika.ConnectionParameters(
    host=os.getenv("RABBIT_HOST","localhost"),
    port=int(os.getenv("RABBIT_PORT","5672")),
    credentials=pika.PlainCredentials(
        os.getenv("RABBIT_USER","guest"),
        os.getenv("RABBIT_PASS","guest")
    )
))
ch = conn.channel()
q = os.getenv("RABBIT_QUEUE_SCRAPER","queue.start_scrapy")
ch.queue_declare(queue=q, durable=True)

body = json.dumps({
    "usuario":"meu@email.com",
    "senha":"minha_senha",
    "tipo de venda":1
})
ch.basic_publish(
    exchange="",
    routing_key=q,
    body=body,
    properties=pika.BasicProperties(delivery_mode=2) # persistente
)
conn.close()
print("mensagem publicada")
```


## üß† Heartbeats e conex√µes longas

Scrapes demorados podem derrubar a conex√£o se heartbeats n√£o forem processados.

No consumer, aumente RABBIT_HEARTBEAT e RABBIT_BLOCKED_TIMEOUT.

No worker, h√° um tick peri√≥dico (process_data_events) controlado por RABBIT_HEARTBEAT_TICK para manter a conex√£o viva durante a execu√ß√£o do spider.

## ü™µ Logs do Scrapy aparecendo como ERROR

O Scrapy loga (INFO/WARNING/‚Ä¶) em stderr. O worker_stream reencaminha preservando o n√≠vel para n√£o marcar tudo como ERROR.