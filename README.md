# ğŸ“¦ Scrapy Servimed

Crawler desenvolvido com [Scrapy](https://scrapy.org/) para fazer login no portal **Servimed**, buscar um id de cliente ativo e coletar informaÃ§Ãµes de produtos gerando dinamicamente um .

## Quais sÃ£o os projetos

Dentro da cada pasta hÃ¡ uma etapa do desafio:

1) ServimedScraper -> abriga o scraper
2) servimedQueue -> abria a fila que recebe o usuÃ¡rio e senha da servimed e faz o scraper no usuÃ¡rio da cote que estiver no env.
3) coteFacilQueue -> Vai receber o pedido e registrar.

## ğŸ” ObservaÃ§Ãµes acerca do teste

AgradeÃ§o a oportunidade de participar do processo seletivo, a prova foi muito bem feita e detalhada e gostei do processo de construir este projeto.

Fiz as requisiÃ§Ãµes 1 a 1 no scraper atÃ© encontrar uma com lista vazia pois isso tornou mais robusto, no primeiro teste que fiz direto Ã  API da servimed o cÃ¡lculo deles de quantidade total de produtos nÃ£o batia com a quantidade de pÃ¡ginas mÃ¡xima.

Sugiro adicionar que o gtin precisa ter pelo menos 8 caracteres e como completar em caso de haver menos. o produto com gtin 450619 tem um cÃ³digo de barras menor do que 8 caracteres.Para entregar a prova eu apenas completei com zeros a esquerda na function  parse_products do spider.

## Um resumo para rodar rapidamente COM DOCKER
Na raiz do projeto rode:
```bash
docker compose up --build -d

```

## Um resumo para rodar rapidamente SEM DOCKER
Se certifique de ter o poetry instalado, entÃ£o clone o projeto:

```bash
git clone https://github.com/gabrielfelipegonso/scrapy-servimed
cd scrapy-servimed
poetry install
```
Existem outras variÃ¡veis de ambiente mas inicialmente configure as qe estÃ£o abaixo e funcionarÃ¡:
```env
RABBIT_HOST=localhost
RABBIT_PORT=5672
RABBIT_USER=guest
RABBIT_PASS=guest
RABBIT_QUEUE_SCRAPER=servimed
API_TOKEN_URL= "https://desafio.cotefacil.net/oauth/token"
API_USERNAME_COTE= "seu_usuario"
API_PASSWORD_COTE= "sua_senha"
RABBIT_QUEUE_PRODUCTS=post_products_cote_facil
API_PRODUCTS_URL="https://desafio.cotefacil.net/produto"
LOG_EVERY_N= 1
LOG_EACH_ITEM = 1
RABBIT_HEARTBEAT=300
RABBIT_BLOCKED_TIMEOUT=600
RABBIT_PREFETCH=1
RABBIT_CONN_ATTEMPTS=5
RABBIT_RETRY_DELAY=5
RABBIT_HEARTBEAT_TICK=1.0
```

Ative seu ambiente virtual, Ã© sÃ³ rodar o comando abaixo, copiar a saÃ­da e colar em seu terminal:

```bash
poetry env activate
```
No ambiente ativo vocÃª pode rodar os dois projetos, hÃ¡ um arquivo dentro da pasta servimedScraper que serve para rodar apenas o scraper e dÃ¡ a possibilidade de vocÃª rodar e salvar em um jsonl os arquivos, pra rodar o scraper separadamente Ã© sÃ³ rodar:

```bash
python run_spider.py   --usuario "meu@email.com"   --senha "minha_senha"   --output produtos.jsonl   --format jsonlines   --loglevel INFO
```
E isso vai gerar um arquivo produtos.jsonl na raiz do scraper, mas vocÃª pode passar outro endereÃ§o caso queira.

Na segunda etapa do desafio Ã© solicitado que rode-se uma fila que espera um usuÃ¡rio e uma senha para comeÃ§ar a receber os produtos. para rodar essa fila Ã© sÃ³ rodar  o script que esta em servimedQueue e chama run_scraper_consumer.py:


```bash
python run_scraper_consumer.py
```


---
# ğŸ•·ï¸ Primeira etapa - Scraper - servimedScraper

## âœ¨ Funcionalidades
- Login com usuÃ¡rio e senha.  
- Descoberta automÃ¡tica do **Client ID** vÃ¡lido.  
- PaginaÃ§Ã£o automÃ¡tica na API de produtos.  
- ExtraÃ§Ã£o de informaÃ§Ãµes de cada produto:
  - **GTIN (EAN)**  
  - **CÃ³digo**  
  - **DescriÃ§Ã£o**  
  - **PreÃ§o de fÃ¡brica**  
  - **Estoque**  
- ExportaÃ§Ã£o dos dados em **JSON**, **JSONLines** ou **CSV**.

---

## ğŸ—‚ Estrutura do projeto

```
scrapy_servimed/
â”œâ”€â”€ scrapy.cfg                  # ConfiguraÃ§Ã£o Scrapy (CLI)
â”œâ”€â”€ run_spider.py               # Script de execuÃ§Ã£o standalone
â”œâ”€â”€ servimedScraper/
â”‚   â”œâ”€â”€ settings.py             # ConfiguraÃ§Ãµes do projeto Scrapy
â”‚   â”œâ”€â”€ middlewares.py          # Middleware custom (auth headers/cookies)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ jwt.py              # FunÃ§Ã£o para decodificaÃ§Ã£o JWT
â”‚   â”‚   â””â”€â”€ requests.py         # Builders de JsonRequest (login, clientes, produtos)
â”‚   â””â”€â”€ spiders/
â”‚       â””â”€â”€ products.py         # Spider principal
â””â”€â”€ tests/                      # Implementar os testes
```

---

## âš™ï¸ Requisitos

- Python 3.12+  
- [Poetry](https://python-poetry.org/) (recomendado) ou `pip`  
- Scrapy >= 2.13  

---

## ğŸš€ InstalaÃ§Ã£o

Clone o repositÃ³rio e instale as dependÃªncias:

```bash
git clone https://github.com/gabrielfelipegonso/scrapy-servimed
cd scrapy-servimed
poetry install
```

Ou, sem Poetry:

```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ ConfiguraÃ§Ã£o de credenciais

As credenciais podem ser fornecidas de duas formas:

### 1. Via linha de comando
```bash
python run_spider.py --usuario "meu@email.com" --senha "minha_senha"
```

### 2. Via variÃ¡veis de ambiente
```bash
# Linux/macOS
export SERVIMED_USER="meu@email.com"
export SERVIMED_PASS="minha_senha"

# Windows (PowerShell)
$env:SERVIMED_USER="meu@email.com"
$env:SERVIMED_PASS="minha_senha"
$env:SERVIMED_SALE_TYPE= '1' ou "2" 
```

---

## ğŸ•·ï¸ Executando o Spider

ApÃ³s ativar o ambiente virtual e acessar no seu terminal a pasta que contÃ©m o script run_spider.py (servimedScraper\servimedScraper)

### OpÃ§Ã£o A â€“ Usando a CLI do Scrapy
```bash
scrapy crawl products -a usuario="meu@email.com" -a senha="minha_senha" -o produtos.jsonl -t jsonlines
```

### OpÃ§Ã£o B â€“ Usando o script `run_spider.py`
```bash
python run_spider.py   --usuario "meu@email.com"   --senha "minha_senha"   --output produtos.jsonl   --format jsonlines   --loglevel INFO
```

---

## ğŸ“Š SaÃ­da dos dadoss

Por padrÃ£o, os produtos sÃ£o exportados em **JSONLines** (`.jsonl`), com um objeto por linha:

```json
{"gtin": "7891234567890", "codigo": "12345", "descricao": "Produto X", "preco_fabrica": 10.5, "estoque": 50}
{"gtin": "7899876543210", "codigo": "67890", "descricao": "Produto Y", "preco_fabrica": 8.0, "estoque": 0}
```

TambÃ©m Ã© possÃ­vel exportar em JSON Ãºnico ou CSV:
```bash
python run_spider.py -o produtos.json -f json
python run_spider.py -o produtos.csv -f csv
```

---
## âš™ï¸ ParÃ¢metros do run_spider.py

O script run_spider.py permite executar o spider diretamente, sem precisar usar o comando scrapy crawl. Ele aceita diversos parÃ¢metros para configurar a execuÃ§Ã£o:

| ParÃ¢metro       | Atalho | Default         | DescriÃ§Ã£o                                                                                                                      |
| --------------- | ------ | ---------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| `--usuario`     | `-u`   | `SERVIMED_USER`  | UsuÃ¡rio de login. Pode ser passado na CLI ou via variÃ¡vel de ambiente.                                                         |
| `--senha`       | `-p`   | `SERVIMED_PASS`  | Senha de login. Pode ser passada na CLI ou via variÃ¡vel de ambiente.                                                           |
| `--output`      | `-o`   | `produtos.jsonl` | Caminho do arquivo de saÃ­da. Aceita qualquer extensÃ£o suportada (`.json`, `.jsonl`, `.csv`).                                   |
| `--format`      | `-f`   | `jsonlines`      | Formato do output (`json`, `jsonlines`, `csv`).                                                                                |
| `--loglevel`    |        | `INFO`           | NÃ­vel de log do Scrapy (`DEBUG`, `INFO`, `WARNING`, `ERROR`).                                                                  |
| `--concurrency` |        | `8`              | NÃºmero mÃ¡ximo de requisiÃ§Ãµes concorrentes (`CONCURRENT_REQUESTS`).                                                             |
| `--delay`       |        | `0.1`            | Atraso (em segundos) entre requisiÃ§Ãµes (`DOWNLOAD_DELAY`).                                                                     |
| `--saleType`    | `-s`   | `1`              | Tipo de venda para as requisiÃ§Ãµes de produtos: `0` (Ã  vista) ou `1` (a prazo). Pode ser definido via env `SERVIMED_SALE_TYPE`. |

## ğŸŒ VariÃ¡veis de Ambiente

AlÃ©m dos parÃ¢metros na linha de comando, vocÃª pode configurar o spider atravÃ©s de variÃ¡veis de ambiente:

| VariÃ¡vel             | Equivalente CLI | DescriÃ§Ã£o                                     |
| -------------------- | --------------- | --------------------------------------------- |
| `SERVIMED_USER`      | `--usuario`     | UsuÃ¡rio de login do portal Servimed.         |
| `SERVIMED_PASS`      | `--senha`       | Senha de login do portal Servimed.                               |
| `SERVIMED_SALE_TYPE` | `--saleType`    | Tipo de venda (`0` = Ã  vista, `1` = a prazo). |

## ğŸ“ Exemplos completos de execuÃ§Ã£o
### 1. Executando com credenciais direto na CLI

```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha
```
### 2. Alterando o formato de saÃ­da
#### Salvando em JSON
```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha -o produtos.json -f json
```
#### Salvando em CSV
```bash
  python run_spider.py -u meu.email@dominio.com -p MinhaSenha -o produtos.csv -f csv
```

### 3;Usando variÃ¡veis de ambiente (sem passar --usuario/--senha)



## ğŸ›  Boas prÃ¡ticas implementadas

- **SeparaÃ§Ã£o de responsabilidades**:
  - Spider cuida apenas do fluxo de scraping.
  - Middleware injeta headers/cookies de autenticaÃ§Ã£o.
  - Utils centralizam a criaÃ§Ã£o de requests (login, clientes, produtos).
- **Uso de `JsonRequest`** para evitar erros de serializaÃ§Ã£o.  
- **State centralizado** (`self.state`) para guardar tokens, cookies e IDs.  
- **Script standalone (`run_spider.py`)** para execuÃ§Ã£o automatizada e integraÃ§Ã£o.  
- **ExportaÃ§Ã£o configurÃ¡vel** (formato/arquivo via parÃ¢metros).  
- **CompatÃ­vel com Scrapy 2.13+** (`async def start`).  

---


# ğŸ“¨ Segunda etapa: Mensageria (RabbitMQ) - servimedQueue

Integra o scraper a um fluxo de mensageria para execuÃ§Ã£o sob demanda e envio do resultado em um Ãºnico POST (array JSON).

## ğŸ¯ Objetivo

Disparar o scraping por mensagem em fila.

Aguardar o spider concluir.

Enviar todos os itens de uma vez para o endpoint (API_PRODUCTS_URL).

Tratar retries/NACK quando a API estiver indisponÃ­vel.

## ğŸ§© Componentes
servimedQueue/
â”œâ”€â”€ consumers/
â”‚   â””â”€â”€ consumer_start_scrapy.py   # Conecta no RabbitMQ e escuta a fila de start
â””â”€â”€ utils/
    â”œâ”€â”€ worker_stream.py           # Callback: roda o spider e faz um POST Ãºnico
    â””â”€â”€ auth.py                    # AuthClient (password grant) para obter o token


O consumer_start_scrapy usa worker_stream.start_scrap, que:

1) executa run_spider.py,
2) acumula itens emitidos via stdout (JSONL), 
3) realiza um POST com o array completo.

ğŸ”„ Fluxo

Publica-se uma mensagem JSON na fila RABBIT_QUEUE_SCRAPER:

```json
{ "usuario": "email@dominio.com", "senha": "secret", "tipo de venda": 1 }
```

O consumer se inicia e chama o worker

O worker executa o spider e coleta cada linha JSON.

Ao finalizar, o worker POSTA um array para API_PRODUCTS_URL usando AuthClient (Bearer token).

ACK sÃ³ apÃ³s POST bem-sucedido.

HTTP 429/5xx â†’ NACK requeue

HTTP 4xx â†’ NACK sem requeue

âš™ï¸ VariÃ¡veis de ambiente (mensageria)
# RabbitMQ
RABBIT_HOST=localhost
RABBIT_PORT=5672
RABBIT_USER=guest
RABBIT_PASS=guest
RABBIT_QUEUE_SCRAPER=queue.start_scrapy
RABBIT_PREFETCH=1

# Timeouts/heartbeats (Ãºtil para scrapes longos)
RABBIT_HEARTBEAT=300
RABBIT_BLOCKED_TIMEOUT=600
RABBIT_CONN_ATTEMPTS=5
RABBIT_RETRY_DELAY=5
RABBIT_HEARTBEAT_TICK=1.0  # frequÃªncia (s) do "tick" de heartbeat no worker

# API destino (POST Ãºnico com array)
API_PRODUCTS_URL=https://sua.api.exemplo/produtos

# Auth (password grant) usados por utils/auth.py
API_TOKEN_URL=https://sso.exemplo/oauth/token
API_USERNAME_COTE=usuario
API_PASSWORD_COTE=senha
API_CLIENT_ID_COTE=
API_CLIENT_SECRET_COTE=
API_SCOPE_COTE=

# Scraper
SERVIMED_SALE_TYPE=1
LOG_LEVEL=INFO

# Logs do worker (opcionais)
LOG_EACH_ITEM=false     # true = loga cada item (verboso)
LOG_EVERY_N=0           # >0 = loga a cada N itens

## â–¶ï¸ Como rodar o consumer
### opÃ§Ã£o A
python servimedQueue/consumers/consumer_start_scrapy.py

### opÃ§Ã£o B (mÃ³dulo)
python -m servimedQueue.consumers.consumer_start_scrapy

## ğŸ“¨ Publicando uma mensagem de teste

```python
import json, os, pika

conn = pika.BlockingConnection(pika.ConnectionParameters(
    host=os.getenv("RABBIT_HOST","localhost"),
    port=int(os.getenv("RABBIT_PORT","5672")),
    credentials=pika.PlainCredentials(
        os.getenv("RABBIT_USER","guest"),
        os.getenv("RABBIT_PASS","guest")
    )
))
ch = conn.channel()
q = os.getenv("RABBIT_QUEUE_SCRAPER","queue.start_scrapy")
ch.queue_declare(queue=q, durable=True)

body = json.dumps({
    "usuario":"meu@email.com",
    "senha":"minha_senha",
    "tipo de venda":1
})
ch.basic_publish(
    exchange="",
    routing_key=q,
    body=body,
    properties=pika.BasicProperties(delivery_mode=2) # persistente
)
conn.close()
print("mensagem publicada")
```


## ğŸ§  Heartbeats e conexÃµes longas

Scrapes demorados podem derrubar a conexÃ£o se heartbeats nÃ£o forem processados.

No consumer, aumente RABBIT_HEARTBEAT e RABBIT_BLOCKED_TIMEOUT.

No worker, hÃ¡ um tick periÃ³dico (process_data_events) controlado por RABBIT_HEARTBEAT_TICK para manter a conexÃ£o viva durante a execuÃ§Ã£o do spider.

## ğŸªµ Logs do Scrapy aparecendo como ERROR

O Scrapy loga (INFO/WARNING/â€¦) em stderr. O worker_stream reencaminha preservando o nÃ­vel para nÃ£o marcar tudo como ERROR.


# ğŸ“¨ TErceira etapa: Mensageria (RabbitMQ) - servimedQueue

Cria pedidos na api cote facil.



## ğŸ§© Componentes
orderQueue/
â”œâ”€â”€ consumers/
â”‚   â””â”€â”€ run_order_consumer.py   # Conecta no RabbitMQ e escuta a fila de pedidos
â”œâ”€â”€ run_prder_consumer.py # Roda o consumer

O run_order_onsumer:

1) LÃª e valida a mensagem
2) Autentica na api usando a lib compartilhada 
3) realiza um POST com o pedido.

ğŸ”„ Fluxo

Publica-se uma mensagem JSON na fila RABBIT_QUEUE_PRODUCTS:

```json
{
"usuario": "fornecedor_user",
"senha": "fornecedor_pass",
"id_pedido": "1234",
"produtos": [
{
"gtin": "1234567890123",
"codigo": "A123",
"quantidade": 1,
}
```

O consumer se inicia, faz as validaÃ§Ãµes, emite os logs e posta o pedido.
