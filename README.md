# üì¶ Scrapy Servimed

Crawler desenvolvido com [Scrapy](https://scrapy.org/) para autenticar no portal **Servimed**, buscar clientes ativos e coletar informa√ß√µes de produtos.

---

## ‚ú® Funcionalidades
- Login com usu√°rio e senha.  
- Descoberta autom√°tica do **Client ID** v√°lido.  
- Pagina√ß√£o autom√°tica na API de produtos.  
- Extra√ß√£o de informa√ß√µes de cada produto:
  - **GTIN (EAN)**  
  - **C√≥digo**  
  - **Descri√ß√£o**  
  - **Pre√ßo de f√°brica**  
  - **Estoque**  
- Exporta√ß√£o dos dados em **JSON**, **JSONLines** ou **CSV**.

---

## üóÇ Estrutura do projeto

```
scrapy_servimed/
‚îú‚îÄ‚îÄ scrapy.cfg                  # Configura√ß√£o Scrapy (CLI)
‚îú‚îÄ‚îÄ run_spider.py               # Script de execu√ß√£o standalone
‚îú‚îÄ‚îÄ servimedScraper/
‚îÇ   ‚îú‚îÄ‚îÄ settings.py             # Configura√ß√µes do projeto Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py          # Middleware custom (auth headers/cookies)
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jwt.py              # Fun√ß√£o para decodifica√ß√£o JWT
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requests.py         # Builders de JsonRequest (login, clientes, produtos)
‚îÇ   ‚îî‚îÄ‚îÄ spiders/
‚îÇ       ‚îî‚îÄ‚îÄ products.py         # Spider principal
‚îî‚îÄ‚îÄ tests/                      # Implementar os testes
```

---

## ‚öôÔ∏è Requisitos

- Python 3.12+  
- [Poetry](https://python-poetry.org/) (recomendado) ou `pip`  
- Scrapy >= 2.13  

---

## üöÄ Instala√ß√£o

Clone o reposit√≥rio e instale as depend√™ncias:

```bash
git clone https://github.com/gabrielfelipeso/scrapy-servimed.git
cd scrapy-servimed
poetry install
```

Ou, sem Poetry:

```bash
pip install -r requirements.txt
```

---

## üîë Configura√ß√£o de credenciais

As credenciais podem ser fornecidas de duas formas:

### 1. Via linha de comando
```bash
python run_spider.py --usuario "meu@email.com" --senha "minha_senha"
```

### 2. Via vari√°veis de ambiente
```bash
# Linux/macOS
export SERVIMED_USER="meu@email.com"
export SERVIMED_PASS="minha_senha"

# Windows (PowerShell)
$env:SERVIMED_USER="meu@email.com"
$env:SERVIMED_PASS="minha_senha"
```

---

## üï∑Ô∏è Executando o Spider

### Op√ß√£o A ‚Äì Usando a CLI do Scrapy
```bash
scrapy crawl products -a usuario="meu@email.com" -a senha="minha_senha" -o produtos.jsonl -t jsonlines
```

### Op√ß√£o B ‚Äì Usando o script `run_spider.py`
```bash
python run_spider.py   --usuario "meu@email.com"   --senha "minha_senha"   --output produtos.jsonl   --format jsonlines   --loglevel INFO
```

---

## üìä Sa√≠da dos dados

Por padr√£o, os produtos s√£o exportados em **JSONLines** (`.jsonl`), com um objeto por linha:

```json
{"gtin": "7891234567890", "codigo": "12345", "descricao": "Produto X", "preco_fabrica": 10.5, "estoque": 50}
{"gtin": "7899876543210", "codigo": "67890", "descricao": "Produto Y", "preco_fabrica": 8.0, "estoque": 0}
```

Tamb√©m √© poss√≠vel exportar em JSON √∫nico ou CSV:
```bash
python run_spider.py -o produtos.json -f json
python run_spider.py -o produtos.csv -f csv
```

---

## üõ† Boas pr√°ticas implementadas

- **Separa√ß√£o de responsabilidades**:
  - Spider cuida apenas do fluxo de scraping.
  - Middleware injeta headers/cookies de autentica√ß√£o.
  - Utils centralizam a cria√ß√£o de requests (login, clientes, produtos).
- **Uso de `JsonRequest`** para evitar erros de serializa√ß√£o.  
- **State centralizado** (`self.state`) para guardar tokens, cookies e IDs.  
- **Script standalone (`run_spider.py`)** para execu√ß√£o automatizada e integra√ß√£o.  
- **Exporta√ß√£o configur√°vel** (formato/arquivo via par√¢metros).  
- **Compat√≠vel com Scrapy 2.13+** (`async def start`).  

---


